Comment produire du code en respectant les bonnes pratiques :

    Gestion des erreurs : Utiliser des blocs try/catch pour capturer et traiter les exceptions.
    Modularit√© :
        D√©couper le code en petites fonctions claires et r√©utilisables.
        √âviter les fonctions trop longues et illisibles.
    Typage des retours :
        Privil√©gier un syst√®me de retour uniformis√© sous forme de tuple : [Error, null] | [null, T] (o√π T est le type attendu en cas de succ√®s).
    Organisation des fichiers :
        S√©parer la logique en plusieurs fichiers selon leur responsabilit√© (ex : services/, utils/, models/).
    Typage et interfaces :
        Isoler les types, interfaces et √©num√©rations (enum) dans des fichiers d√©di√©s (ex : types.ts, interfaces/, enums.ts).
    Tests unitaires :
        Cr√©er un fichier de test pour chaque fonction/m√≥dulo d√©velopp√© (ex : maFonction.test.ts).
        Utiliser un framework comme Jest, Mocha ou Vitest.
    Suivi des progr√®s :
        Documenter l‚Äôavancement du d√©veloppement dans un fichier PROGRESS.md (ex : fonctionnalit√©s impl√©ment√©es, bugs corrig√©s, d√©cisions techniques).
    Jeux de donn√©es pour les tests :
        Utiliser les fichiers du dossier /dataset pour les tests et l‚Äôentra√Ænement.
        Cr√©er des donn√©es suppl√©mentaires si n√©cessaire pour couvrir tous les cas d‚Äôusage.


Travaux du jour:
- Cr√©ation du type `Result<T>` et helpers `ok`/`fail` (`src/types/result.ts`).
- Statistiques descriptives: `computeMean`, `computeMedian`, `computeModes`, `computeVariance`, `computeStandardDeviation`, `computeQuantile`, `computeSummary` (`src/stats/*`).
- Utilitaires texte: `stripHtml`, `tokenize` (`src/text/tokenize.ts`).
- Fr√©quence des mots (TF): `termFrequency` avec options relatif/brut (`src/text/tf.ts`).
- Tests: `src/stats/descriptive_test.ts`, `src/text/tf_test.ts`.
- D√©mo: `main.ts` ex√©cute un r√©sum√© statistique et un TF simple.
 - IDF: `idfFromDocs` avec lissage et base log personnalisable (`src/text/idf.ts`).
 - Tests IDF: `src/text/idf_test.ts` (corpus simple, smoothing > 0).
 - D√©mo mise √† jour: `main.ts` lit `dataset/pieceoccasion-1.html` et `dataset/pieceoccasion-2.html`, calcule TF sur `text1` et un √©chantillon d'IDF sur les deux documents.
 - TF-IDF: `tfidfFromDocs` et helper `topKTerms` (`src/text/tfidf.ts`).
 - Tests TF-IDF: `src/text/tfidf_test.ts`.
 - D√©mo TF-IDF: `main.ts` affiche le top 10 des termes TF-IDF pour chaque document.
 - Cooccurrence: `buildCooccurrenceMatrix`, `mostAssociated`, `computePPMI` (`src/text/cooccurrence*.ts`).
 - Tests Cooccurrence/PPMI: `src/text/cooccurrence_test.ts`.
 - D√©mo Cooccurrence: `main.ts` affiche voisins de `"porte"` (brut et PPMI).
 - PCA: `pca`, `pcaTransform` (`src/stats/pca.ts`) + utilitaires matrice (`src/math/matrix.ts`).
 - Analyse factorielle: `factorAnalysis` avec rotation varimax (`src/stats/factor.ts`).
 - Tests PCA/FA: `src/stats/pca_test.ts`, `src/stats/factor_test.ts`.
 - D√©mo PCA/FA: `main.ts` calcule TF-IDF sur 2 docs et affiche top termes par composante/facteur.

Session du 04/10/2025 - Normalisation HTML:
 - Types de normalisation: `NormalizationStrategy` (enum), `NormalizeOptions`, `NormalizedContent`, `HTML_ENTITIES` (`src/text/normalize_types.ts`).
 - Fonctions de normalisation HTML (`src/text/normalize.ts`):
   * `normalizeHtml`: Point d'entr√©e principal avec s√©lection de strat√©gie
   * `normalizeBasic`: Suppression simple des balises HTML
   * `normalizeContentOnly`: Garde uniquement le contenu visible (enl√®ve scripts, styles, commentaires, SVG)
   * `normalizeStructureAware`: Pr√©serve la structure avec sauts de ligne pour titres/paragraphes
   * `normalizeWithMetadata`: Extrait m√©tadonn√©es (title, description, keywords, language) + contenu
   * `normalizeAggressive`: Nettoyage maximal pour texte pur uniquement
   * `extractText`: Helper simple pour extraction de texte
   * `compareStrategies`: Compare toutes les strat√©gies sur un m√™me HTML
 - Tests unitaires complets: `src/text/normalize_test.ts` (30+ tests couvrant toutes les strat√©gies et cas limites).
 - Exemples d'utilisation: `examples/normalize_example.ts` (8 exemples d√©taill√©s incluant pipeline complet).
 - D√©cisions techniques:
   * Utilisation du type `Result<T>` pour gestion uniforme des erreurs
   * Enum `NormalizationStrategy` pour s√©lection explicite de strat√©gie
   * Support Unicode complet (caract√®res accentu√©s, chinois, arabe, etc.)
   * D√©codage des entit√©s HTML (nomm√©es, d√©cimales, hexad√©cimales)
   * Options configurables pour whitespace, lignes vides, sauts de ligne
   * Fonction helper `decodeHtmlEntities` pour conversion d'entit√©s HTML courantes
 - Cas d'usage recommand√©s par strat√©gie:
   * BASIC: Tests rapides, prototypage
   * CONTENT_ONLY: Usage g√©n√©ral, recommand√©e par d√©faut
   * STRUCTURE_AWARE: Analyse de structure documentaire, extraction de sections
   * WITH_METADATA: SEO, classification de documents, enrichissement contextuel
   * AGGRESSIVE: Nettoyage maximal pour analyse linguistique pure

Session du 04/10/2025 - Sprint 0: Pr√©paration du Projet (PLAN_FINAL.md):
 - ‚úÖ Plan de d√©veloppement final consolid√© (fusion des plans A et BIS) : `PLAN_FINAL.md` (1726 lignes)
 - ‚úÖ Structure de dossiers cr√©√©e pour nouveaux modules:
   * `src/html/` - Parsing HTML et manipulation DOM
   * `src/extraction/` - Extraction donn√©es produit
   * `src/classification/` - Classification pages produit vs non-produit
   * `src/pipeline/` - Pipeline unifi√© d'analyse
   * `src/cli/` - Interface ligne de commande
   * `tests/integration/` - Tests d'int√©gration
   * `tests/benchmarks/` - Benchmarks de performance
   * `tools/` - Outils annexes
 - ‚úÖ Configuration `deno.json` mise √† jour:
   * D√©pendance `linkedom@^0.16` ajout√©e (parser DOM HTML)
   * D√©pendances `@std/path`, `@std/fs` ajout√©es
   * TypeScript strict mode activ√©
   * Tasks ajout√©es: test, bench, fmt, lint
 - ‚úÖ Fichiers de types cr√©√©s (1,106 lignes total):
   * `src/html/parser_types.ts` (154 lignes): DOMNode, ParseOptions, ParsedDocument, JsonLdData, MicrodataItem, PageMetadata
   * `src/extraction/extraction_types.ts` (340 lignes): ProductInfo (35+ champs), Money, Weight, Dimensions, BatteryInfo, ExtractionEvidence, FusionResult
   * `src/classification/classification_types.ts` (324 lignes): PageFeatures, ClassificationResult, ClassifierRules, ScoringReport, ClassificationMetrics
   * `src/pipeline/analyzer_types.ts` (288 lignes): AnalysisOptions, AnalysisResult, BatchOptions, OutputFormat, PipelineConfig
 - ‚úÖ Validation:
   * Compilation TypeScript: ‚úì Tous les fichiers compilent sans erreur
   * Lint: ‚úì Aucun warning (correction de `any` ‚Üí `unknown`)
   * linkedom: ‚úì Test√© et fonctionnel
 - üìã D√©cisions techniques Sprint 0:
   * Utilisation de `linkedom` comme parser DOM (l√©ger, performant, compatible Deno)
   * Architecture modulaire: html/ ‚Üí extraction/ ‚Üí classification/ ‚Üí pipeline/
   * Types strictement typ√©s avec `unknown` au lieu de `any`
   * Normalisation rigoureuse: prix en centimes (ISO 4217), poids en grammes, dimensions en millim√®tres
   * Evidence tracking complet pour tra√ßabilit√© des extractions
   * Feature engineering riche: structural (16 features), textual (12 features), semantic (6 features)
   * Pipeline configurable avec steps optionnels
   * Support multi-format: JSON, CSV, Markdown, texte
 - üéØ Pr√™t pour Sprint 1: Parsing & Extraction de base (3 jours)